# -*- coding: utf-8 -*-
"""Edited_Capstone_draft.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qcgsPm_c5P8Iw-Y84jdzyjZtV7MTnrfG

# CAPSTONE PROJECT

Group 4 collaborators:

- Tony Munene
- Loise Mbago
- Immanuel Omondi
- Nazra Nyangwara
- Bahati Ndwiga
- Cleophas Opati
- Stephen Gathai - Team Lead

# DEEP LEARNING IN BREAST CANCER DETECTION

## 1.0 Business Understanding

### 1.1 Background

Breast cancer ranks as the most common cancer globally and the second leading cause of cancer-related deaths. According to the World Health Organization (WHO), breast cancer is the primary cause of mortality among women aged between 45-55 years with an incidence of 1 in 8 women. Early detection and timely intervention play a pivotal role in the prognosis of breast cancer. When detected early, and if adequate diagnosis and treatment are available, the chances of survival increase significantly. Thus, the importance of early detection cannot be overstated, as it can lead to more effective treatments, reduced intervention procedures, and ultimately, lower the risk of breast-cancer-related deaths considerably. Mammography and ultrasound are the primary imaging modalities for the early breast cancer diagnosis. However, these modalities are limited in the extent to which they facilitate early diagnosis. First, follow-up ultrasound, aspiration, or biopsy may need to be performed after a breast ultrasound image is interpreted to determine whether a suspicious abnormality is cancer. Often, most of the suspected problem locations detected by ultrasonography that require biopsy are noncancerous (false positives). Second, many tumors cannot be detected via ultrasonography and annual mammography is still recommended. However, in mammography, some early breast tumors simply manifest as calcifications and go undetected. Therefore, more sophisticated tools are required to overcome the limitations of ultrasound in the early detection of breast cancer.

### 1.2 Problem Statement
The Breast Cancer Research Foundation(BCRF) is seeking to advance breast cancer research by harnessing deep learning to transform early detection and intervention in breast cancer treatment and management. They have engaged our services to design and implement a cutting-edge Deep Learning-Based Breast Cancer Prediction System (DL-BCPS) capable of analyzing medical imaging data, including mammograms and ultrasound images to accurately predict the presence of breast cancer in patients. Through the DL-BCPS, BCRF aims to revolutionize breast cancer research, significantly enhancing both the precision and efficiency of breast cancer diagnosis and providing healthcare professionals with a valuable tool to facilitate timely medical intervention, improve patient outcomes, and support decision-making.

### 1.3 Objectives
- To develop a deep learning model using medical imaging data capable of efficient segmentation of breast masses in ultrasound images.

- To determine specific parameters and features within the medical imaging data that are most indicative of early-stage breast cancer to improve the model's ability to detect cancer at its earliest stages.

- To establish a timeline for medical intervention based on the model's predictions, ensuring that cases with a high likelihood of breast cancer are flagged promptly for further examination and diagnosis.

- To implement a user-friendly interface for healthcare professionals to upload medical images and receive predictions from the DL-BCPS.

### 1.4 Success Criteria
The model should:
- Achieve a classification accuracy of at least 85% on the test dataset.
- Achieve a sensitivity of at least 95% and a specificity of at least 90%
- Have a high F1-score to ensure a balance between precision and recall.
- Demonstrate good generalization by performing well on unseen data.

## 2.0 Data Understanding

The [Breast Ultrasound Images Dataset](https://scholar.cu.edu.eg/?q=afahmy/pages/dataset) includes breast ultrasound images from 600 female patients aged between 25 and 75 years. The dataset consists of 780 images with an average image size of 500 Ã— 500 pixels. The images are in PNG format.

The data is categorized into three sets:

- Benign: This set contains 437 images.
- Malignant: This set contains 210 images.
- Normal: This set contains 133 images.
"""

#pip install --upgrade numpy

#pip install --upgrade h5py

"""# Importing relevant libraries
Below libraries will be  used in machine learning and computer vision tasks, Overall, these libraries and modules provide a comprehensive set of tools for data handling, visualization, model building, and evaluation, making it possible to develop an effective breast cancer predictive model.
"""

# Import relevant libraries
import os
import random
import warnings
from itertools import chain
import cv2
import glob
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import pandas as pd
import tensorflow as tf
from PIL import Image
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import accuracy_score, recall_score, precision_score
from sklearn.model_selection import train_test_split
from tensorflow.keras import regularizers
from tensorflow.keras.applications import DenseNet121, InceptionV3, MobileNetV2, ResNet50, VGG16
from tensorflow.keras.applications.mobilenet_v2 import preprocess_input
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.layers import (BatchNormalization, Conv2D, Dense, Dropout, Flatten, GlobalAveragePooling2D, MaxPooling2D)
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import to_categorical

"""#Mounting drive and Loading the various classes of images ."""

# Load the dataset from the drive
from google.colab import drive
drive.mount('/content/drive')

# Defining paths to the directories
normal_path = '/content/drive/MyDrive/Breast_cancer_dataset/normal'
benign_path = '/content/drive/MyDrive/Breast_cancer_dataset/benign'
malignant_path = '/content/drive/MyDrive/Breast_cancer_dataset/malignant'
image_dir = '/content/drive/MyDrive/Breast_cancer_dataset'

"""#Preview sample images
Overall, below code snippet serves as a quick visual inspection of sample images from each category in your breast cancer dataset. It helps  verify that the image loading and categorization process is functioning correctly and gives  a visual sense of the data/images  that we will be working with. This is an essential step in the data exploration and preprocessing phase when building a machine learning model.
"""

# Preview sample images

# Create dictionary for categories and paths
categories_paths = {
    'normal': normal_path,
    'benign': benign_path,
    'malignant': malignant_path
}

# Randomly sample 5 images from each category
normal_images = [os.path.join(normal_path, fname) for fname in random.sample(os.listdir(normal_path), 5)]
benign_images = [os.path.join(benign_path, fname) for fname in random.sample(os.listdir(benign_path), 5)]
malignant_images = [os.path.join(malignant_path, fname) for fname in random.sample(os.listdir(malignant_path), 5)]

# Plot images
fig, axes = plt.subplots(3, 5, figsize=(15, 15))
fig.suptitle('Ultrasound Images', fontsize=16, y=1.05)

# Display benign images
for ax, img_path in zip(axes[0], normal_images):
    img = mpimg.imread(img_path)
    ax.imshow(img, cmap='gray')
    ax.set_title("Normal")
    ax.axis('off')

# Display malignant images
for ax, img_path in zip(axes[1], benign_images):
    img = mpimg.imread(img_path)
    ax.imshow(img, cmap='gray')
    ax.set_title("Benign")
    ax.axis('off')

# Display normal images
for ax, img_path in zip(axes[2], malignant_images):
    img = mpimg.imread(img_path)
    ax.imshow(img, cmap='gray')
    ax.set_title("Malignant")
    ax.axis('off')

plt.tight_layout()
plt.show()

"""***Observations:***
- The images are already in grayscale hence we will not need to convert them to grayscale
- The images are in different sizes and will need to be resized

#Checking for missing and or Corrupt Images

The provided code defines a function called check_images(directory) and uses it to check for corrupt or missing image files in three directories corresponding to different categories (normal, benign, and malignant).

Importance:

1.This code is essential for data quality control and ensures that the image dataset is clean and ready for further processing. Corrupt or missing images can lead to issues during model training and evaluation.
2.Identifying corrupt or missing files helps maintain the integrity of the dataset and allows you to decide whether to exclude or replace problematic images.
3.By using the verify method from Pillow (Image.verify()), the code checks if the images are valid and complete, which is crucial for preventing errors during image loading and processing.
4.It provides a diagnostic tool to quickly identify and address data quality issues, ultimately contributing to the reliability and robustness of any machine learning model built using this dataset.
"""

# Check for missing or corrupt images

def check_images(directory):

    corrupt_files = []
    # List all files in the directory
    files = os.listdir(directory)
    for file in files:
        file_path = os.path.join(directory, file)
        try:
            # Try to open the image
            with Image.open(file_path) as img:
                # Check if the image is complete
                img.verify()
        except (IOError, SyntaxError) as e:
            corrupt_files.append(file_path)

    return corrupt_files

# Check each directory for corrupt or missing images
corrupt_normal = check_images(normal_path)
corrupt_benign = check_images(benign_path)
corrupt_malignant = check_images(malignant_path)

# Output the corrupt or missing files
corrupt_files = corrupt_normal + corrupt_benign + corrupt_malignant
if corrupt_files:
    print("Corrupt or missing files:")
    for file_path in corrupt_files:
        print(f" - {file_path}")
else:
    print("No corrupt or missing files found.")

"""### Image sizes

In this section, we analyze the dimensions of the images in the dataset. Understanding the variation in image sizes will inform the decision on whether to resize images for model training and what target dimensions to use.

**Objectives:**

The code's main objective is to provide an understanding of the distribution of image sizes within each category of the breast cancer dataset. This is important because images with significantly different sizes may require preprocessing (resizing, cropping) before being used as inputs to a machine learning model.

It assists in identifying potential anomalies or outliers in image sizes, which could indicate data quality issues or errors in the dataset.

The visualizations help data analysts and model developers gain insights into the dataset's characteristics, which can inform decisions related to data preprocessing, model architecture, and training strategies.

By visualizing the image size distribution, one can ensure that the dataset is suitable for further analysis and modeling and thus  make informed decisions about how to handle images of varying sizes during the model-building process.
"""

# Initialize lists to store widths and heights
widths = []
heights = []

# Loop over each category and path
for category, path in categories_paths.items():
    for img_name in os.listdir(path):
        img_path = os.path.join(path, img_name)
        # Read the image in grayscale
        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        # Get the dimensions
        h, w = img.shape
        widths.append(w)
        heights.append(h)

# Plot the Image Size Distribution
fig, axes = plt.subplots(1, 2, figsize=(12, 5))
axes[0].hist(widths, bins=20, color='blue', edgecolor='black')
axes[0].set_title('Image Width Distribution')
axes[0].set_xlabel('Width')
axes[0].set_ylabel('Frequency')

axes[1].hist(heights, bins=20, color='green', edgecolor='black')
axes[1].set_title('Image Height Distribution')
axes[1].set_xlabel('Height')
axes[1].set_ylabel('Frequency')

plt.tight_layout()
plt.show()

"""# Class distribution

In summary, these lines of code help us to  quickly assess the class distribution within the  breast cancer dataset. Understanding the distribution is crucial because an imbalanced dataset (where one class has significantly fewer samples than others) can affect the performance and bias of machine learning models. Visualizing the distribution with a bar chart makes it easy to see if any class requires oversampling, undersampling, or other techniques to address class imbalance.
"""

# Get the list of files in each directory
normal_images = os.listdir(normal_path)
benign_images = os.listdir(benign_path)
malignant_images = os.listdir(malignant_path)

# Count the number of images in each class
num_normal_images = len(normal_images)
num_benign_images = len(benign_images)
num_malignant_images = len(malignant_images)

# Print the counts
print("Number of images for each class:")
print("Normal:", num_normal_images)
print("Benign:", num_benign_images)
print("Malignant:", num_malignant_images)

# Plot a bar chart to visualize the distribution
classes = ['Normal', 'Benign', 'Malignant']
counts = [num_normal_images, num_benign_images, num_malignant_images]

plt.bar(classes, counts)
plt.xlabel('Class')
plt.ylabel('Number of Images')
plt.title('Distribution of Images in Each Class')
plt.show()

"""Observation:
- The output above reveals a noticeable class imbalance in the dataset. The benign category is significantly overrepresented compared to the normal and malignant categories. This imbalance will need to be addressed to ensure the model provides a reliable and generalized performance across all classes.

### Mask analysis

In summary, these lines of code are essential for the preprocessing and feature extraction stage of a breast cancer predictive model. They apply masks to isolate potential cancerous regions within medical images, which is an essential  step in extracting relevant features for training a machine learning model to classify breast cancer images accurately. The visual analysis of masks helps assess the quality of this preprocessing step.

By applying mask analysis to normal images, one can assess how well the masking technique distinguishes between cancerous and non-cancerous regions. It's a crucial step in the development of a breast cancer predictive model because it helps in identifying and isolating the relevant features for classification.
"""

# Define function to apply masks to original images
def apply_masks(images):
    masked_images = []
    for img in images:
        masked_img = cv2.bitwise_and(img, img, mask=img)
        masked_images.append(masked_img)
    return masked_images

def mask_analysis(image_path):
    try:
        # Get image files in the specified path
        image_files = [os.path.join(image_path, f) for f in os.listdir(image_path) if f.endswith('.png')]

        # Load images in grayscale
        images = [cv2.imread(img_f, cv2.IMREAD_GRAYSCALE) for img_f in image_files]

        # Apply adaptive thresholding to isolate cancerous regions
        masks = []
        for img in images:
            mask = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2)
            masks.append(mask)

            # Set all non-cancerous regions to black
            mask[mask != 255] = 0

        # Display original images and masked images for a sample of three images
        for i in range(3):
            plt.figure(figsize=(12, 4))
            plt.subplot(1, 3, 1)
            plt.imshow(images[i], cmap='gray')
            plt.title('Original Image')

            plt.subplot(1, 3, 2)
            plt.imshow(masks[i], cmap='gray')
            plt.title('Generated Mask')

            plt.show()
    except Exception as e:
        print(f"Error processing images in {image_path}: {e}")

# Display masks of normal images
mask_analysis(normal_path)

# Display masks of benign images
mask_analysis(benign_path)

# Display masks of malignant images
mask_analysis(malignant_path)

"""## 3.0 Data Preparation

In this section, we will perform several preprocessing steps on the images to prepare them for training. This includes:
- Resizing the images to a consistent size
- Normalizing the pixel values to a range between 0 and 1
- Creating labels for each set
- Addressing class imbalance in the dataset
- Applying data augmentation techniques to increase the variability and size of the training dataset

### a. Resize and normalize images

In summary, these lines of code are crucial for the data preparation and exploration phase of building a breast cancer predictive model. They ensure that the images are in the correct format and size for model input and provide a visual check to confirm the quality of the preprocessing steps. This is a critical step in the overall process of training and evaluating a machine learning model.
"""

# Resize images to 224x224 pixels
img_size = (224, 224)

def load_images(directory):
    images = []
    for fname in os.listdir(directory):
        filepath = os.path.join(directory, fname)
        img = Image.open(filepath)
        img = img.resize(img_size)
        img_array = np.array(img) / 255.0  # Normalize to [0, 1]
        images.append(img_array)
    return np.array(images)

#Display images after resizing and normalizing

# Load images from each category
normal_images = load_images(normal_path)
benign_images = load_images(benign_path)
malignant_images = load_images(malignant_path)

# Display the first image from each category
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

axes[0].imshow(normal_images[0])
axes[0].set_title('Normal')
axes[0].axis('off')

axes[1].imshow(benign_images[0])
axes[1].set_title('Benign')
axes[1].axis('off')

axes[2].imshow(malignant_images[0])
axes[2].set_title('Malignant')
axes[2].axis('off')

plt.tight_layout()
plt.show()

"""# b. Image feature analysis

Feature extraction is crucial in image analysis because it helps represent images in a more meaningful and concise manner. Shape features, in particular, can be important for distinguishing between different objects or regions of interest within images. These features provide quantitative information about the shape and structure of objects, which can be valuable for classification tasks.In medical image analysis, features play a critical role in the automatic detection and classification of diseases or abnormalities, such as breast cancer.
"""

# Extract shape features from the images
def extract_shape_features(image):
    # Convert the image to float32 if it's in float64
    if image.dtype == np.float64:
        image = image.astype(np.float32)

    # Ensure the image is in grayscale format
    if len(image.shape) == 3:
        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # Convert the image to uint8 type and scale it to 0-255 if necessary
    if image.dtype != np.uint8:
        image = (image * 255).astype(np.uint8)

    # Apply binary thresholding
    _, thresh = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)

    # Find contours
    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # If no contours are detected, return None
    if not contours:
        return None

    # Get the largest contour (assuming it's the main object in the image)
    contour = max(contours, key=cv2.contourArea)

    # Extract shape features
    area = cv2.contourArea(contour)
    perimeter = cv2.arcLength(contour, True)
    try:
        ellipse = cv2.fitEllipse(contour)
        (center, axes, orientation) = ellipse
        major_axis_length = max(axes)
        minor_axis_length = min(axes)
        eccentricity = np.sqrt(1 - (minor_axis_length / major_axis_length)**2)
    except:
        eccentricity = 0

    hull = cv2.convexHull(contour)
    hull_area = cv2.contourArea(hull)
    solidity = float(area) / hull_area

    equivalent_diameter = np.sqrt(4 * area / np.pi)

    return {
        "Area": area,
        "Perimeter": perimeter,
        "Eccentricity": eccentricity,
        "Solidity": solidity,
        "Equivalent Diameter": equivalent_diameter
    }

# Display the first image from each category with shape features
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

images = [normal_images[0], benign_images[0], malignant_images[0]]
titles = ['Normal', 'Benign', 'Malignant']

for ax, img, title in zip(axes, images, titles):
    shape_features = extract_shape_features(img)
    display_text = "\n".join([f"{key}: {value:.2f}" for key, value in shape_features.items()])

    ax.imshow(img, cmap='gray')
    ax.set_title(title)
    ax.axis('off')
    ax.text(0, 224, display_text, color="red", backgroundcolor="white")

plt.tight_layout()
plt.show()

"""Feature interpretation:
- **Area**: The benign image has a much larger area compared to both the normal and malignant images. This might suggest that benign tumors or abnormalities are often larger or more spread out in the image than their malignant counterparts. This could also be influenced by the fact that benign tumors grow more slowly and may be detected at a larger size.

- **Perimeter**: The malignant image has the smallest perimeter. This could suggest that malignant tumors or abnormalities may have more irregular, spiky boundaries, leading to a smaller perimeter relative to their area.

- **Eccentricity**: Eccentricity values are closer to 1 for more elongated shapes and closer to 0 for more circular shapes. Both normal and malignant images have high eccentricity values, indicating that they may be more elongated or irregular in shape compared to the benign image which is more circular.

- **Solidity**: A higher solidity value indicates a smoother and more compact shape. The benign image has the highest solidity, suggesting it has a more compact and solid shape, whereas the normal image is more porous or has more internal holes.

- **Equivalent diameter**: The benign image, consistent with its large area, has the largest equivalent diameter. The malignant image, despite having a larger area than the normal image, has a diameter that's not drastically different, possibly due to its irregular shape.

### c. Image data augmentation

Data augmentation is important for several reasons:
It increases the effective size of the training dataset, which can help improve the generalization and performance of machine learning models.
By applying random transformations (e.g., rotations, shifts, zooms) to the images, it makes the model more robust to variations and distortions that may be present in real-world data.
It helps mitigate the risk of overfitting by providing the model with more diverse examples during training. Monitoring the number of augmented images ensures that the dataset remains balanced and that we have a sufficient amount of augmented data for training.
"""

# Define function to perform data augmentation
def apply_augmentations(image_list, datagen):
    augmented_images = []
    for img_path in image_list:
        img = cv2.imread(img_path)
        if img is None:  # Check if the image was loaded successfully
            continue

        # Reshape the image to fit the expected input shape of datagen.flow
        img = img.reshape((1,) + img.shape)

        # Generate 20 augmented images for each input image
        for _ in range(10):
            for batch in datagen.flow(img, batch_size=1):
                augmented_image = batch[0]
                # Resize the image to ensure consistency
                augmented_image = cv2.resize(augmented_image, (224, 224))
                augmented_images.append(augmented_image.astype(np.uint8))
                break

    return np.array(augmented_images)

# Construct the list of image paths based on image directories
normal_images_paths = glob.glob(os.path.join(normal_path, '*.png'))
malignant_images_paths = glob.glob(os.path.join(malignant_path, '*.png'))
benign_images_paths = glob.glob(os.path.join(benign_path, '*.png'))

all_image_paths = normal_images_paths + malignant_images_paths + benign_images_paths

# Define the parameters for Data Augmentation
datagen = ImageDataGenerator(
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.1,
    zoom_range=0.1,
    fill_mode='nearest'
)

# Apply augmentations
augmented_images = apply_augmentations(all_image_paths, datagen)
augmented_images = [cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) for img in augmented_images]


print(f"Total number of augmented images: {len(augmented_images)}")

""" # Display augmented images alongside original images
In summary, this code is important for visually inspecting and assessing the impact of data augmentation on the dataset. It helps verify that the augmentation techniques are adding valuable variability to the data without distorting or degrading the information required for the breast cancer classification task. This visual analysis is essential in the data preprocessing and model development process to ensure that augmented data remains informative and suitable for training the intended  machine learning model.
"""

# Determine the number of images to display (the minimum of 6, length of images, and length of augmented_images)
num_images = min(6, len(images), len(augmented_images))

# Visualize original images alongside augmented images
plt.figure(figsize=(15, 10))

for i in range(num_images):
    # Original Image
    plt.subplot(num_images, 2, i*2 + 1)
    plt.imshow(images[i], cmap='gray')
    plt.axis('off')
    plt.title(f'Original Image {i+1}')

    # Augmented Image
    plt.subplot(num_images, 2, i*2 + 2)
    plt.imshow(augmented_images[i], cmap='gray')
    plt.axis('off')
    plt.title(f'Augmented Image {i+1}')

plt.tight_layout()
plt.show()

"""# d. Create labels

Labels are values or categories assigned to each data point in a dataset to represent its class or category. Labels are used in supervised learning to train machine learning models. In classification tasks, each data point is associated with a label that indicates its class or category.

Importance of Labels:

Supervision and Learning:  They provide the model with the correct answers during training, enabling it to learn patterns and make predictions.

Model Evaluation:  They enable us to  compare the model's predictions to the ground truth and assess its accuracy and effectiveness.

Decision Making: In applications like medical diagnosis, labels are crucial for making informed decisions based on model predictions.

Understanding Data: Labels provide context and meaning to the data. They help interpret and understand the characteristics and distribution of different classes in the dataset.

 Assigned  labels to each category of images:
0 is assigned to normal images.
1 is assigned to benign images.
2 is assigned to malignant images.
"""

# Factor for the number of augmented images generated per original image
augmentation_factor = 10

# Create labels: 0 for normal, 1 for benign, 2 for malignant
normal_labels = np.zeros((len(normal_images_paths) * augmentation_factor,), dtype=int)
benign_labels = np.ones((len(benign_images_paths) * augmentation_factor,), dtype=int)
malignant_labels = np.full((len(malignant_images_paths) * augmentation_factor,), 2)

# Concatenate data and labels:

# Resize each image in X to the shape (224, 224)
X_images = [cv2.resize(img, (224, 224)) for img in augmented_images]

# Convert list of images back to numpy array
X = np.array(X_images)

# Expand dimensions to get the shape (7800, 224, 224, 1)
X = np.expand_dims(X, axis=-1)

# Combine the separate label arrays into a single array 'y' that holds all the labels
y = np.concatenate([normal_labels, benign_labels, malignant_labels])

X.shape

y.shape

"""**Intrepretations in summary:**

(7800, 224, 224, 1) describes the dimensions of the image data array. It means there are  7,800 grayscale images, each with dimensions of 224 pixels in height and 224 pixels in width, and each image has a single color channel (grayscale).
(7800,) under y.shape describes the dimensions of the label array. It means there are  7,800 labels, one for each image in the dataset. These labels are used to indicate the class or category to which each image belongs, and they are in a one-dimensional array.

### e. Splitting the dataset

In this subsection, we divide our dataset into training, validation, and test sets in the ratios 7:1:2 respectively.

In summary, these lines of code are crucial for proper dataset management and model development. They ensure that the data is divided into training, validation, and test sets while maintaining the class distribution, allowing for the training and evaluation of machine learning models in a well-structured manner.
"""

# Define the ratios
train_ratio = 0.7
validation_ratio = 0.10
test_ratio = 0.20

# # Calculate the sizes of the validation and test sets
# validation_size = validation_ratio / (validation_ratio + test_ratio)
# test_size = test_ratio / (validation_ratio + test_ratio)

# Split the data into a training set and a temporary set for validation and testing
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=1 - train_ratio, random_state=42, stratify=y
)

# Split the temporary set into validation and test sets
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=test_ratio, random_state=42, stratify=y_temp
)

# Output the shapes to verify the splits
X_train.shape, X_val.shape, X_test.shape

y_train.shape

"""***In summary, these shapes provide information about the size and characteristics of each dataset split:***

Training set: 5,459 grayscale images with dimensions 224x224 pixels.
Validation set: 1,872 grayscale images with dimensions 224x224 pixels.
Test set: 469 grayscale images with dimensions 224x224 pixels.
These splits are essential for training, fine-tuning, and evaluating machine learning models for breast cancer classification.

# 4.0 Modeling

## 4.1 Baseline CNN Model

In summary, below lines of code define, configure, and provide an overview of a CNN model tailored for breast cancer classification. The model's architecture, compilation, and summary are fundamental aspects of building and understanding deep learning models for image classification tasks.

Defining the model architecture is a critical step in deep learning. The chosen architecture, including the number and types of layers, directly impacts the model's ability to learn and make accurate predictions.

Compiling the model configures it for training by specifying the optimization algorithm, loss function, and evaluation metrics.

Reviewing the model summary provides insights into the model's complexity and helps ensure that it is constructed as intended.

Model summary helps in debugging, fine-tuning, and understanding the neural network's architecture. It's a crucial tool for model development.
"""

# Define the model
baseline_model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 1)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(3, activation='softmax')
])

# Compile the model
baseline_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Review the model summary
baseline_model.summary()

"""**Interpretation and Implications:**

The model consists of two convolutional layers followed by max-pooling layers for feature extraction. These layers are used to automatically learn relevant features from the input images.
The flatten layer reshapes the output from the convolutional layers into a one-dimensional vector for further processing.
The two dense layers, particularly the last one, are used for classification. The final dense layer has three output units, which suggests that this is a multi-class classification model with three classes.
The model has a large number of trainable parameters (23,907,203), which allows it to capture complex patterns in the data.
Having a high number of parameters also increases the risk of overfitting, so regularization techniques may be necessary during training.
The output shape of the final dense layer ((None, 3)) matches the number of classes in the classification task (3 classes). *italicized text*
In terms of reliability:

The model appears to be sufficiently complex to learn from the data, but careful training and validation are needed to ensure it generalizes well to new, unseen data.
The architecture is a good starting point for a classification task, but model performance will also depend on data quality, preprocessing, and training parameters.
Regularization techniques and hyperparameter tuning should be considered to optimize the model's performance and reliability.
Overall, this model provides a foundation for breast cancer classification, and further training and evaluation are required to assess its reliability and effectiveness.

# Fit the model

Significance of fitting the model;

These lines of code are essential for the actual training of the CNN model, which is the core step in developing the intended  machine learning model.
Data augmentation helps the model become more robust by exposing it to a variety of data variations during training, reducing the risk of overfitting.
Validation data is used to assess the model's performance on unseen data and make informed decisions about model adjustments.
The number of epochs (70 in this case) indicates  the  model will iterate 70 times over the training data to refine its parameters.
In summary, these lines of code are crucial for training and fine-tuning the CNN model, ensuring that it learns to make accurate predictions on breast cancer images while avoiding overfitting. The results variable is typically used to track and analyze the model's training progress, including metrics such as accuracy and loss over epochs.
"""

# Fit the model
results = baseline_model.fit(
    datagen.flow(X_train, y_train, batch_size=64),
    validation_data=(X_val, y_val),
    epochs=70
)

"""The provided output represents the training progress and performance metrics of the breast cancer predictive model (CNN) during training. Let's interpret these results:

**Epochs:** The training process is carried out for a total of 70 epochs.

**Loss:** The loss is a measure of how well the model is performing. It indicates the error between the predicted values and the actual values (labels). The goal is to minimize this value.

**Accuracy**: Accuracy represents the fraction of correctly classified samples out of the total samples. It is a measure of how well the model is making correct predictions.

**Val_loss**: Val_loss (Validation Loss) is the loss on the validation dataset. It measures how well the model generalizes to unseen data. A decreasing val_loss indicates that the model is learning from the training data and is likely improving its performance.

**Val_accuracy**: Val_accuracy (Validation Accuracy) is the accuracy on the validation dataset. It measures how well the model performs on unseen data. An increasing val_accuracy indicates improved generalization.

#Interpretation of  the training progress:

The initial training loss is very high, indicating that the model's predictions are far from the actual labels.
As training progresses, both training loss and validation loss decrease. This suggests that the model is learning from the data and improving its predictions.
Training accuracy and validation accuracy also improve over epochs, indicating that the model is becoming more accurate in classifying breast cancer images.
Validation accuracy (val_accuracy) on unseen data is particularly important as it measures the model's ability to generalize to new, unseen cases.

**Conclusions;**

The decreasing loss values indicate that the model is learning to make better predictions as training progresses.
The increasing accuracy values suggest that the model is becoming more proficient at classifying breast cancer images.
Monitoring validation metrics (val_loss and val_accuracy) is crucial to ensure that the model generalizes well to unseen data.

Overall, the training process is effective, as both training and validation metrics are improving. The final validation accuracy (val_accuracy) of 0.8317 indicates that the model can accurately classify breast cancer images with an approximately 83.17% accuracy on unseen data. This is a positive outcome.

# Visualize loss and Accuracy metrics

Below  code is meant to visualize the training and validation loss and accuracy over a series of epochs for a breast cancer predictive model. This type of visualization is common in deep learning and machine learning to assess the performance of a model during training.

Importance and Objectives:

Monitoring Overfitting: By comparing the training and validation metrics, one can identify if the model is overfitting. Overfitting occurs when the model performs well on the training data but poorly on the validation data.
Assessing Model Performance: The plots allow the practitioner to quickly assess how well the model is doing, both in terms of loss (how wrong its predictions are) and accuracy (how often it is correct).

Determining Convergence: By examining the plots, one can see if the metrics have stabilized or if they are still changing significantly. This can be useful to determine if more training epochs might be beneficial or if the training can be stopped early.

Hyperparameter Tuning: These visualizations can be crucial when adjusting hyperparameters. By observing how changes in hyperparameters affect the loss and accuracy, one can make informed decisions on the best configurations for the model.
"""

# Visualize loss and accuracy metrics
training_loss = results.history['loss']
validation_loss = results.history['val_loss']
training_accuracy = results.history['accuracy']
validation_accuracy = results.history['val_accuracy']

# Plot loss
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.plot(training_loss, label='Training Loss')
plt.plot(validation_loss, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss')
plt.legend()

# Plot accuracy
plt.subplot(1, 2, 2)
plt.plot(training_accuracy, label='Training Accuracy')
plt.plot(validation_accuracy, label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

"""**Interpretation:**

The model is learning and improving its predictions over time, as indicated by the decreasing loss and increasing accuracy.

# Evaluate the model
In summary, the provided code evaluates the performance of baseline_model on a test dataset and prints out the loss and accuracy. This step is essential in machine learning to ensure the model's robustness and readiness for practical applications.
"""

test_loss, test_accuracy = baseline_model.evaluate(X_test, y_test)
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")

"""# Interpretations

15/15: This indicates that the model evaluated the test dataset in 15 batches. Every batch contains a subset of the test data. Once all 15 batches are processed, the entire test dataset has been evaluated.
1s 77ms/step: On average, it took approximately 77 milliseconds to process each batch, and the entire evaluation process for all 15 batches took around 1 second.
loss: 0.4839: The average loss value for the test dataset is 0.4839. The loss is a measure of how well the model's predictions match the actual outcomes. A lower loss indicates better model performance.
accuracy: 0.8081: This is the accuracy metric, and it shows that the model correctly predicted the outcomes for about 80.81% of the test samples.

# Predicting  classes for validation set and Printing the classification report;

The provided code is centered around predicting the classes for a validation set using a trained model and then evaluating the quality of these predictions using a classification report

**Importance and Objectives:**

Model Evaluation: The primary objective of these lines is to evaluate the performance of the trained model on a validation set. This helps in understanding how well the model is likely to perform on unseen data.

Quality Check: The classification report provides detailed metrics that can help identify areas where the model is strong or weak. For instance, if the model has high precision but low recall for a particular class, it indicates that the model is conservative in predicting that class but is usually correct when it does.

Guiding Model Improvement: By understanding the strengths and weaknesses from the classification report, one can make informed decisions on how to improve the model, whether it's gathering more data, changing the model architecture, or tuning hyperparameters.

Stakeholder Communication: Classification reports can also be useful when communicating the model's performance to stakeholders, as they provide a comprehensive view of its strengths and weaknesses.

In summary, the provided code predicts class labels for a validation set using a trained model and then evaluates the quality of these predictions in a detailed manner. This is essential for understanding, communicating, and improving the model's performance.
"""

# Predict classes for validation set
y_pred = baseline_model.predict(X_val)
y_pred_classes = np.argmax(y_pred, axis=1)

# Print the classification report
print(classification_report(y_val, y_pred_classes))

"""**This is a classification report that provides a detailed evaluation of the model's performance on the dataset:**

Classes (0, 1, 2): These represent the labels or categories the model is predicting. In the context of breast cancer, these could represent stages or types of cancer, or they might represent 'benign', 'malignant', and 'normal', or some other classification. The exact meaning would depend on the dataset and the problem formulation.

Precision: This metric gives the ratio of correctly predicted positive observations to the total predicted positives. Higher precision indicates that the false positive rate is lower.

For class 0, it's 0.85, meaning 85% of the samples predicted as class 0 were actually class 0.
Recall (or Sensitivity): This metric gives the ratio of correctly predicted positive observations to all the observations in the actual class.

For class 0, it's 0.82, meaning the model correctly identified 82% of all actual class 0 samples.
F1-Score: The F1-Score is the weighted average of Precision and Recall. It tries to balance the trade-off between precision and recall.

For class 0, it's 0.83, providing a singular metric that balances both precision and recall for this class.
Support: This gives the number of actual occurrences of the class in the dataset.

There are 319 samples of class 0, 1049 samples of class 1, and 504 samples of class 2.
Accuracy: This is the ratio of correctly predicted observation to the total observations. Here, the model's overall accuracy is 0.83 or 83%, meaning it made correct predictions for 83% of all samples.

Macro Avg: This provides the average scores (precision, recall, and f1-score) across all classes, treating all classes equally regardless of their size or prevalence in the dataset.

Weighted Avg: This provides the average scores (precision, recall, and f1-score) across all classes, but it takes into account the number of samples in each class (i.e., it's weighted by class size).

Interpretation:

The model shows a reasonably good performance across all three classes, with an overall accuracy of 83%.
Precision, recall, and F1-score are relatively consistent across the classes, indicating that the model does not heavily favor one class over the others.
Depending on the clinical importance and implications of each class, one might prioritize improving precision or recall for specific classes. For instance, in a medical context, having a high recall for a malignant class might be prioritized to ensure fewer false negatives.
In the context of breast cancer prediction, these results provide confidence in the model's ability to classify samples into the respective categories. However, the exact implications and next steps would also depend on the specific objectives of the project and the clinical significance of each class.

#Generate confusion matrix & plot the confusion matrix

The code  is for generating and visualizing a confusion matrix for the model's predictions on the validation dataset.

The confusion matrix is a summary of the predictions made by a classification model. It is used to understand the performance of the algorithm, whether it is a binary or a multi-class classifier.

In this case, the matrix will show the number of times each class ('Normal', 'Benign', 'Malignant') was predicted for each true class.

It's especially helpful for identifying where the model is making mistakes - for example, how many times it confused 'Benign' tumors for 'Malignant', which can be crucial in a medical diagnosis context.
"""

# Generate the confusion matrix
cm = confusion_matrix(y_val, y_pred_classes)

# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='g', cmap='Blues',
            xticklabels=['Normal', 'Benign', 'Malignant'],
            yticklabels=['Normal', 'Benign', 'Malignant'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

"""# Interpretation of the provided confusion matrix:

True Normal (Actual Normal cases):

Predicted Normal: 260 cases were correctly identified as Normal.
Predicted Benign: 40 cases that were actually Normal were incorrectly identified as Benign.
Predicted Malignant: 19 cases that were actually Normal were incorrectly identified as Malignant.
True Benign (Actual Benign cases):

Predicted Normal: 34 cases that were actually Benign were incorrectly identified as Normal.
Predicted Benign: 914 cases were correctly identified as Benign.
Predicted Malignant: 101 cases that were actually Benign were incorrectly identified as Malignant.
True Malignant (Actual Malignant cases):

Predicted Normal: 13 cases that were actually Malignant were incorrectly identified as Normal.
Predicted Benign: 108 cases that were actually Malignant were incorrectly identified as Benign.
Predicted Malignant: 383 cases were correctly identified as Malignant.
Key Insights:

The diagonal values (260, 914, 383) represent the number of correct predictions for each class, which is a good sign as these numbers are relatively high.

The number of Normal cases misclassified as Benign or Malignant is relatively low (59 in total), suggesting the model can reasonably distinguish normal samples.

In the context of breast cancer prediction, it's crucial to correctly identify malignant cases to ensure timely medical interventions. Therefore, while the overall accuracy might seem high, the misclassifications between Benign and Malignant are crucial and might necessitate further model refinement or additional diagnostic tests in a clinical setting.

## 4.2 Baseline model with added layers

Model Architecture: Convolutional Neural Networks (CNNs) excel at tasks involving image data due to their ability to capture spatial hierarchies of features. The provided model structure, with its convolutional and dense layers, is tailored to process image data effectively.
Regularization: Layers like Dropout and techniques like BatchNormalization help prevent overfitting and ensure stable and efficient training.
Early Stopping: In deep learning, models can sometimes continue to learn without significant improvements, leading to wasted computational resources. Early stopping ensures efficient training by halting once improvements stagnate.
Model Compilation: Before training, the model needs to be compiled with an optimizer, loss function, and evaluation metric. This step prepares the model for training on specific data.
In summary, this code sets up a CNN model tailored for image classification, likely for distinguishing between different stages or types of breast cancer based on medical imagery. The architecture and components are chosen to ensure effective learning, generalization, and computational efficiency.
"""

# Define the model
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

model_1 = Sequential([
    # First convolutional block
    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 1)),
    BatchNormalization(),
    MaxPooling2D(2, 2),
    Dropout(0.25),

    # Second convolutional block
    Conv2D(64, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D(2, 2),
    Dropout(0.25),

    # Third convolutional block
    Conv2D(128, (3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D(2, 2),
    Dropout(0.25),

    # Flatten and fully connected layers
    Flatten(),
    Dense(512, activation='relu'),
    BatchNormalization(),
    Dropout(0.5),

    # Output layer
    Dense(3, activation='softmax')
])

# Compile the model
model_1.compile(optimizer = Adam(learning_rate=0.001),
              loss = 'sparse_categorical_crossentropy',
              metrics = ['accuracy'])

# Review the model summary
model_1.summary()

"""#Fit the model

Callbacks are functions that can be applied at given stages of the training process. Here, the early_stopping callback is used, which was defined earlier. It will monitor the validation loss and stop training if it doesn't see improvement for a set number of epochs, ensuring efficient training and preventing overfitting.  In summary, this code trains the CNN model on augmented image data, monitors its performance on validation data, and ensures efficient and effective training using the early stopping mechanism.
"""

# Fit the model
results_1 = model_1.fit(
    datagen.flow(X_train, y_train, batch_size=64),
    validation_data=(X_val, y_val),
    epochs=70,
    callbacks = [early_stopping]
)

# Visualize loss and accuracy metrics
training_loss = results_1.history['loss']
validation_loss = results_1.history['val_loss']
training_accuracy = results_1.history['accuracy']
validation_accuracy = results_1.history['val_accuracy']

# Plot loss
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.plot(training_loss, label='Training Loss')
plt.plot(validation_loss, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss')
plt.legend()

# Plot accuracy
plt.subplot(1, 2, 2)
plt.plot(training_accuracy, label='Training Accuracy')
plt.plot(validation_accuracy, label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

# Evaluate the model
test_loss, test_accuracy = model_1.evaluate(X_test, y_test)
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")

# Predict classes for validation set
y_pred_1 = model_1.predict(X_val)
y_pred_classes_1 = np.argmax(y_pred_1, axis=1)

# Print the classification report
print(classification_report(y_val, y_pred_classes_1))

# Generate the confusion matrix
cm = confusion_matrix(y_val, y_pred_classes_1)

# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='g', cmap='Blues',
            xticklabels=['Normal', 'Benign', 'Malignant'],
            yticklabels=['Normal', 'Benign', 'Malignant'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

"""## 4.3 Pretrained model: VGG16"""

from keras.models import Model

vgg = VGG16(input_shape = (224, 224, 1), weights=None, include_top=False)

for layer in vgg.layers:
  layer.trainable = False

x = Flatten()(vgg.output)
output = Dense(3, activation='softmax')(x)

model_2 = Model(inputs=vgg.input, outputs=output)
model_2.compile(
  loss= 'sparse_categorical_crossentropy',
  optimizer='adam',
  metrics=['accuracy']
)
model_2.summary()

# Fit the model
results_2 = model_2.fit(datagen.flow(
    X_train, y_train, batch_size=64),
    validation_data=(X_val, y_val),
    epochs=70
)

# Visualize loss and accuracy metrics
training_loss = results_2.history['loss']
validation_loss = results_2.history['val_loss']
training_accuracy = results_2.history['accuracy']
validation_accuracy = results_2.history['val_accuracy']

# Plot loss
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.plot(training_loss, label='Training Loss')
plt.plot(validation_loss, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss')
plt.legend()

# Plot accuracy
plt.subplot(1, 2, 2)
plt.plot(training_accuracy, label='Training Accuracy')
plt.plot(validation_accuracy, label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

# Evaluate the model
test_loss, test_accuracy = model_2.evaluate(X_test, y_test)
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")

# Predict classes for validation set
y_pred_2 = model_2.predict(X_val)
y_pred_classes_2 = np.argmax(y_pred_2, axis=1)

# Print the classification report
print(classification_report(y_val, y_pred_classes_2))

# Generate the confusion matrix
cm = confusion_matrix(y_val, y_pred_classes_2)

# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='g', cmap='Blues',
            xticklabels=['Normal', 'Benign', 'Malignant'],
            yticklabels=['Normal', 'Benign', 'Malignant'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

"""## 5.0 Model Evaluation

## 5.1 Model comparison
"""

models = [baseline_model, model_1, model_2]
model_names = ["baseline_model", "model_1", "model_2"]
classes = ['normal', 'benign', 'malignant']

# Define a result table as a DataFrame
columns = ['models'] + [f"accuracy_{cls}" for cls in classes] + [f"recall_{cls}" for cls in classes]
result_table = pd.DataFrame(columns=columns)

# Evaluate the models and record the results
for idx, model in enumerate(models):
    model_name = model_names[idx]
    y_pred = model.predict(X_val)
    y_pred_classes = np.argmax(y_pred, axis=1)

    # Overall accuracy
    accuracy = accuracy_score(y_val, y_pred_classes)

    # Class-wise recall
    recalls = recall_score(y_val, y_pred_classes, average=None)

    # Construct the results row
    results = {'models': model_name}
    results.update({f"accuracy_{cls}": accuracy for cls in classes})
    results.update({f"recall_{cls}": recalls[i] for i, cls in enumerate(classes)})

    result_table = result_table.append(results, ignore_index=True)

# Set the name of the models as index labels
result_table.set_index('models', inplace=True)

result_table

"""## 5.1 Hyperparameter tuning the best model"""

# # Define the model creation function
# def create_model(optimizer='adam'):
#     model = Sequential([
#         Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 1)),
#         MaxPooling2D((2, 2)),
#         Conv2D(64, (3, 3), activation='relu'),
#         MaxPooling2D((2, 2)),
#         Flatten(),
#         Dense(128, activation='relu'),
#         Dense(3, activation='softmax')
#     ])
#     model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
#     return model

# # Define hyperparameters
# optimizers = ['adam']
# batch_sizes = [16, 32, 64]
# epochs_list = [30, 40, 50, 100]

# # Define early stopping
# early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)

# # Grid search
# best_score = 0
# best_params = None

# for optimizer in optimizers:
#     for batch_size in batch_sizes:
#         for epochs in epochs_list:
#             model = create_model(optimizer)
#             history = model.fit(datagen.flow(X_train, y_train, batch_size=batch_size),
#                                 epochs=epochs,
#                                 validation_data=(X_val, y_val),
#                                 callbacks=[early_stopping],
#                                 verbose=0)
#             val_accuracy = max(history.history['val_accuracy'])
#             print(f"Optimizer: {optimizer}, Batch Size: {batch_size}, Epochs: {epochs}, Validation Accuracy: {val_accuracy:.4f}")

#             if val_accuracy > best_score:
#                 best_score = val_accuracy
#                 best_params = (optimizer, batch_size, epochs)

# print("\\nBest Parameters:")
# print(f"Optimizer: {best_params[0]}, Batch Size: {best_params[1]}, Epochs: {best_params[2]}, Best Validation Accuracy: {best_score:.4f}")

# Tune the parameters on the best performing model
epochs = 50
batch_size=32

# Fit the best performing model
best_results = baseline_model.fit(
    datagen.flow(X_train, y_train, batch_size=batch_size),
    validation_data=(X_val, y_val),
    epochs=epochs
)

# Visualize loss and accuracy metrics
training_loss = best_results.history['loss']
validation_loss = best_results.history['val_loss']
training_accuracy = best_results.history['accuracy']
validation_accuracy = best_results.history['val_accuracy']

# Plot loss
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.plot(training_loss, label='Training Loss')
plt.plot(validation_loss, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss')
plt.legend()

# Plot accuracy
plt.subplot(1, 2, 2)
plt.plot(training_accuracy, label='Training Accuracy')
plt.plot(validation_accuracy, label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

# Evaluate the model
test_loss, test_accuracy = baseline_model.evaluate(X_test, y_test)
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")

# Predict classes for validation set
y_pred_3 = baseline_model.predict(X_val)
y_pred_classes_3 = np.argmax(y_pred_3, axis=1)

# Print the classification report
print(classification_report(y_val, y_pred_classes_3))

# Generate the confusion matrix
cm = confusion_matrix(y_val, y_pred_classes_3)

# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='g', cmap='Blues',
            xticklabels=['Normal', 'Benign', 'Malignant'],
            yticklabels=['Normal', 'Benign', 'Malignant'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

# Save the model
model.save('baseline_model.h5')